{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'H', 'N', 'O', 'F'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures = pd.read_csv('../../data/structures.csv')\n",
    "structures.atom.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class util:\n",
    "    @staticmethod\n",
    "    def mkdirs(paths):\n",
    "        if isinstance(paths, list) and not isinstance(paths, str):\n",
    "            for path in paths:\n",
    "                util.mkdir(path)\n",
    "        else:\n",
    "            util.mkdir(paths)\n",
    "\n",
    "    @staticmethod\n",
    "    def mkdir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "atomtypes: element\n",
      "batch_size: 8\n",
      "beta1: 0.9\n",
      "checkpoints_dir: ./checkpoints\n",
      "continue_train: False\n",
      "dataroot: ../../data\n",
      "epoch_count: 1\n",
      "gpu_ids: [0]\n",
      "grid_func: None\n",
      "grid_size: 9.6\n",
      "grid_spacing: 0.2\n",
      "init_type: normal\n",
      "input_nc: 5\n",
      "isTrain: True\n",
      "lr: 0.0001\n",
      "lr_decay_iters: 50\n",
      "lr_policy: lambda\n",
      "max_dataset_size: 153353\n",
      "model: simple\n",
      "momentum: 0.9\n",
      "nThreads: 4\n",
      "name: experiment_name\n",
      "niter: 3\n",
      "niter_decay: 3\n",
      "norm: instance\n",
      "nx: 47\n",
      "ny: 47\n",
      "nz: 47\n",
      "optimizer: adam\n",
      "phase: train\n",
      "print_freq: 100\n",
      "rotate: 0.0\n",
      "save_epoch_freq: 5\n",
      "save_latest_freq: 5000\n",
      "serial_batches: False\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class BaseOptions:\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "        self.initialized = False\n",
    "    \n",
    "    def initialize(self):\n",
    "        self.parser.add_argument('--dataroot', required=True, help='path to csv files')\n",
    "        self.parser.add_argument('--batch_size', type=int, default=1, help='input batch size')\n",
    "        self.parser.add_argument('--checkpoints_dir', default='./checkpoints', help='models are saved here')\n",
    "        self.parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
    "        self.parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
    "        self.parser.add_argument('--nThreads', default=6, type=int, help='# threads for loading data')\n",
    "        self.parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"),\n",
    "                                 help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
    "        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
    "        self.parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
    "        self.parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization')\n",
    "\n",
    "        # custom\n",
    "        self.parser.add_argument('--atomtypes', type=str, default='element',\n",
    "                                 help='atom typing policy. [element]')\n",
    "        self.parser.add_argument('--model', type=str, default='simple',\n",
    "                                 help='chooses which model to use. [simple]')\n",
    "        self.parser.add_argument('--grid_size', type=float, default=24, help='size of grid (angstrom)')\n",
    "        self.parser.add_argument('--grid_spacing', type=float, default=1.0, help='spacing of grid')\n",
    "        self.parser.add_argument('--grid_func', type=str, help='atom density function')\n",
    "        self.parser.add_argument('--rotate', type=float, default=180, help='random rotation of dataset')\n",
    "        self.initialized = True\n",
    "\n",
    "    def parse(self, args=None):\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        self.opt = self.parser.parse_args(args)\n",
    "        self.opt.isTrain = self.isTrain\n",
    "        \n",
    "        str_ids = self.opt.gpu_ids.split(',')\n",
    "        self.opt.gpu_ids = []\n",
    "        for str_id in str_ids:\n",
    "            id = int(str_id)\n",
    "            if id >= 0:\n",
    "                self.opt.gpu_ids.append(id)\n",
    "\n",
    "        # set gpu ids\n",
    "        if len(self.opt.gpu_ids) > 0:\n",
    "            torch.cuda.set_device(self.opt.gpu_ids[0])\n",
    "            \n",
    "        # input channel\n",
    "        if self.opt.atomtypes == 'element':\n",
    "            self.opt.input_nc = 5\n",
    "        \n",
    "        self.opt.nx, self.opt.ny, self.opt.nz = [int(self.opt.grid_size/self.opt.grid_spacing) for _ in range(3)]\n",
    "        if self.opt.nx % 2 == 0: self.opt.nx += 1\n",
    "        if self.opt.ny % 2 == 0: self.opt.ny += 1\n",
    "        if self.opt.nz % 2 == 0: self.opt.nz += 1\n",
    "            \n",
    "        args = vars(self.opt)\n",
    "\n",
    "        print('------------ Options -------------')\n",
    "        for k, v in sorted(args.items()):\n",
    "            print('%s: %s' % (str(k), str(v)))\n",
    "        print('-------------- End ----------------')\n",
    "        \n",
    "        # save to the disk\n",
    "        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n",
    "        util.mkdirs(expr_dir)\n",
    "        file_name = os.path.join(expr_dir, 'opt.txt')\n",
    "        with open(file_name, 'wt') as opt_file:\n",
    "            opt_file.write('------------ Options -------------\\n')\n",
    "            for k, v in sorted(args.items()):\n",
    "                opt_file.write('%s: %s\\n' % (str(k), str(v)))\n",
    "            opt_file.write('-------------- End ----------------\\n')\n",
    "        return self.opt\n",
    "\n",
    "class TrainOptions(BaseOptions):\n",
    "    def initialize(self):\n",
    "        BaseOptions.initialize(self)\n",
    "        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--optimizer', type=str, default='adam', help='optimizer [sgd|adam]')\n",
    "        self.parser.add_argument('--momentum', type=float, default=0.9, help='momentum term')\n",
    "        self.parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n",
    "        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')\n",
    "        self.parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n",
    "        self.parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
    "        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        self.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
    "        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n",
    "        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n",
    "        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
    "        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
    "        self.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
    "        self.isTrain = True\n",
    "\n",
    "class TestOptions(BaseOptions):\n",
    "    def initialize(self):\n",
    "        BaseOptions.initialize(self)\n",
    "        self.parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n",
    "        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
    "        self.isTrain = False\n",
    "        \n",
    "opt = TrainOptions().parse('--dataroot ../../data --grid_size 9.6 --grid_spacing 0.2 \\\n",
    "                            --batch_size 8 --rotate 0 \\\n",
    "                            --max_dataset_size 153353 --nThreads 4 \\\n",
    "                            --lr 0.0001 --optimizer adam \\\n",
    "                            --niter 3 --niter_decay 3'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 47, 47)\n",
      "-4.612698136270047 -3.514195895195008 -4.591999004036189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff02f5ed550>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numba\n",
    "from math import exp, sqrt, cos, sin, e\n",
    "\n",
    "@numba.jit('f4[:,:,:](f4[:,:], f4[:,:,:], i8, i8, i8, f8, f8, f8, f8, f8)', nopython=True)\n",
    "def coords_to_grid_numba(coords, grid, nx, ny, nz, xmin, ymin, zmin, spacing, rvdw):\n",
    "    exps = 0.01\n",
    "    rmax = 20\n",
    "    #expt = np.exp(-(rvdw/np.arange(0,rmax,exps))**12)\n",
    "    expt = np.exp(-(rvdw/np.arange(0,rmax,exps))**12)\n",
    "    nc = len(coords)\n",
    "    for i in range(nx):\n",
    "        ix = xmin + i*spacing\n",
    "        for j in range(ny):\n",
    "            iy = ymin + j*spacing\n",
    "            for k in range(nz):\n",
    "                iz = zmin + k*spacing\n",
    "                for l in range(nc):\n",
    "                    dx = ix - coords[l,0]\n",
    "                    dy = iy - coords[l,1]\n",
    "                    dz = iz - coords[l,2]\n",
    "                    r = sqrt(dx*dx + dy*dy + dz*dz)\n",
    "                    #grid[i,j,k] += 1 - exp(-(rvdw/r)**12)\n",
    "                    if r > rmax: continue\n",
    "                    grid[i,j,k] += 1 - expt[int(r/exps)]\n",
    "    return grid\n",
    "\n",
    "nx = ny = nz = int(opt.grid_size / opt.grid_spacing)\n",
    "if nx % 2 == 0: nx += 1\n",
    "if ny % 2 == 0: ny += 1\n",
    "if nz % 2 == 0: nz += 1\n",
    "grid = np.zeros((nx, ny, nz), dtype=np.float32)\n",
    "print(grid.shape)\n",
    "coords = np.array(((-0.012698135900000001, 1.085804158, 0.008000995799999999),\n",
    "                   ( 2.15041600e-03, -6.03131760e-03,  1.97612040e-03)), dtype=np.float32)\n",
    "spacing = opt.grid_spacing\n",
    "xmin = coords[0][0] - (nx-1)/2*spacing\n",
    "ymin = coords[0][1] - (ny-1)/2*spacing\n",
    "zmin = coords[0][2] - (nz-1)/2*spacing\n",
    "print(xmin, ymin, zmin)\n",
    "rvdw = 1\n",
    "grid = coords_to_grid_numba(coords, grid, nx, ny, nz, xmin, ymin, zmin, spacing, rvdw)\n",
    "\n",
    "x = np.arange(nx)\n",
    "y = grid[int((nx-1)/2), :, int((nx-1)/2)]\n",
    "plt.xlabel('Y-axis')\n",
    "plt.ylabel('grid value')\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from math import exp, sqrt, cos, sin, e\n",
    "\n",
    "def get_transform(opt):\n",
    "    transform_list = [Center()]\n",
    "    if opt.rotate > 0:\n",
    "        transform_list.append(Rotate(opt.rotate))\n",
    "        \n",
    "    if opt.atomtypes == 'element':\n",
    "        for at in sorted(['C', 'H', 'N', 'O', 'F']):\n",
    "            transform_list.append(Voxelize(at, opt.nx, opt.ny, opt.nz, opt.grid_spacing, opt.grid_func))\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    transform_list += [ToTensor(opt)]\n",
    "    return transforms.Compose(transform_list)\n",
    "    \n",
    "class Voxelize:\n",
    "    \"\"\"Voxelize coordinates per atomtype\"\"\"\n",
    "    def __init__(self, atomtype_filters, nx, ny, nz, grid_spacing, grid_func=None):\n",
    "        self.atomtype_filters = atomtype_filters\n",
    "        self.grid_spacing = float(grid_spacing)\n",
    "        self.rvdw = float(1)\n",
    "        self.grid_func = grid_func\n",
    "        self.nx, self.ny, self.nz = nx, ny, nz\n",
    "    \n",
    "    def __call__(self, item):\n",
    "        xmin = item['center'][0] - (self.nx - 1) / 2 * self.grid_spacing\n",
    "        ymin = item['center'][1] - (self.ny - 1) / 2 * self.grid_spacing\n",
    "        zmin = item['center'][2] - (self.nz - 1) / 2 * self.grid_spacing\n",
    "        grid = np.zeros((self.nx, self.ny, self.nz), dtype=np.float32)\n",
    "        if isinstance(self.atomtype_filters, str):\n",
    "            mask = item['atomtypes'] == self.atomtype_filters\n",
    "        else:\n",
    "            pass\n",
    "        if self.grid_func == None:\n",
    "            grid = coords_to_grid_numba(item['coords'][mask], grid,\n",
    "                                        self.nx, self.ny, self.nz, \n",
    "                                        xmin, ymin, zmin, \n",
    "                                        self.grid_spacing, self.rvdw)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        if 'channels' not in item:\n",
    "            item['channels'] = [grid]\n",
    "        else:\n",
    "            item['channels'].append(grid)\n",
    "        return item\n",
    "\n",
    "class Rotate:\n",
    "    \"\"\"Rotate input structure\"\"\"\n",
    "    def __init__(self, degree):\n",
    "        self.degree = degree\n",
    "    \n",
    "    def __call__(self, item, angles=None):\n",
    "        if angles is None:\n",
    "            angles = (np.random.random_sample(3,) - 0.5)*self.degree/180*np.pi\n",
    "        \n",
    "        # http://mathworld.wolfram.com/EulerAngles.html\n",
    "        phi, theta, psi = angles\n",
    "        cphi, ctheta, cpsi = np.cos(angles)\n",
    "        sphi, stheta, spsi = np.sin(angles)\n",
    "        r = np.array(((cpsi*cphi - ctheta*sphi*spsi, cpsi*sphi + ctheta*cphi*spsi, spsi*stheta),\n",
    "                      (-spsi*cphi - ctheta*sphi*cpsi, -spsi*sphi + ctheta*cphi*cpsi, cpsi*stheta),\n",
    "                      (stheta*sphi, -stheta*cphi, ctheta)))\n",
    "        item['coords'] = np.array(np.dot(r, item['coords'].T).T, dtype=np.float32)\n",
    "        \n",
    "        # tensor\n",
    "        tensor = item['tensor'].reshape((3, 3))\n",
    "        item['tensor'] = np.array(np.dot(r, tensor), dtype=np.float32).flatten()\n",
    "        return item\n",
    "\n",
    "class Center:\n",
    "    \"\"\"Center input structure\"\"\"\n",
    "    def __call__(self, item):\n",
    "        com = item['center']\n",
    "        item['coords'] -= com\n",
    "        item['center'] = np.array((0, 0, 0))\n",
    "        return item\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, item):\n",
    "        channels = np.vstack([c[np.newaxis,:] for c in item['channels']])\n",
    "        return {\n",
    "            'channels': torch.from_numpy(channels),\n",
    "            'tensor': torch.from_numpy(item['tensor'])\n",
    "        }\n",
    "\n",
    "# test\n",
    "data = {'atomtypes': np.array(['C', 'H', 'H', 'H', 'H']), \n",
    "        'coords': np.array([[-1.26981359e-02,  1.08580416e+00,  8.00099580e-03],\n",
    "                            [ 2.15041600e-03, -6.03131760e-03,  1.97612040e-03],\n",
    "                            [ 1.01173084e+00,  1.46375116e+00,  2.76574800e-04],\n",
    "                            [-5.40815069e-01,  1.44752661e+00, -8.76643715e-01],\n",
    "                            [-5.23813635e-01,  1.43793264e+00,  9.06397294e-01]]), \n",
    "        'tensor': np.array([ 1.95315e+02,  0.00000e+00, -1.00000e-04,  7.00000e-04,\n",
    "                             1.95317e+02,  7.00000e-04, -1.00000e-04,  7.00000e-04,\n",
    "                             1.95317e+02]), \n",
    "        'center': np.array([-0.01269814,  1.08580416,  0.008001  ])}\n",
    "\n",
    "import copy\n",
    "sample = copy.copy(data)\n",
    "c = Center()\n",
    "r = Rotate(90)\n",
    "grid_size = opt.grid_size\n",
    "grid_spacing = opt.grid_spacing\n",
    "v = Voxelize('C', opt.nx, opt.ny, opt.nz, grid_spacing)\n",
    "\n",
    "item = c(sample)\n",
    "assert np.abs(np.sum(sample['coords'][0])) < 1e-6 # should be close to zero after center\n",
    "data = copy.copy(sample)\n",
    "\n",
    "rot1 = r(sample, np.deg2rad((90, 0, 0)))\n",
    "rot2 = r(rot1, np.deg2rad((-90, 0, 0)))\n",
    "assert np.all(np.abs(data['coords'] - rot2['coords']) < 1e-4), data['coords'] - rot2['coords']\n",
    "assert np.all(np.abs(data['tensor'] - rot2['tensor']) < 1e-4), data['tensor'] - rot2['tensor']\n",
    "\n",
    "rot1 = r(sample, np.deg2rad((0, 90, 0)))\n",
    "rot2 = r(rot1, np.deg2rad((0, -90, 0)))\n",
    "assert np.all(np.abs(data['coords'] - rot2['coords']) < 1e-4), data['coords'] - rot2['coords']\n",
    "assert np.all(np.abs(data['tensor'] - rot2['tensor']) < 1e-4), data['tensor'] - rot2['tensor']\n",
    "\n",
    "rot1 = r(sample, np.deg2rad((0, 0, 90)))\n",
    "rot2 = r(rot1, np.deg2rad((0, 0, -90)))\n",
    "assert np.all(np.abs(data['coords'] - rot2['coords']) < 1e-4), data['coords'] - rot2['coords']\n",
    "assert np.all(np.abs(data['tensor'] - rot2['tensor']) < 1e-4), data['tensor'] - rot2['tensor']\n",
    "\n",
    "channels = v(item)['channels']\n",
    "assert channels[0].shape[0] == opt.nx\n",
    "ci = int((nx-1)/2)\n",
    "assert channels[0][ci, ci, ci] == 1.0, channels[0][ci, ci, ci]\n",
    "print(opt.nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "from torch.utils import data\n",
    "import os\n",
    "import time\n",
    "\n",
    "class ChampsDataset(data.Dataset):\n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.transform = get_transform(opt)\n",
    "        self.dataroot = opt.dataroot\n",
    "\n",
    "        structures_csvfile = os.path.join(self.dataroot, 'structures.csv')\n",
    "        tensors_csvfile = os.path.join(self.dataroot, 'magnetic_shielding_tensors.csv')\n",
    "        self.structures = pd.read_csv(structures_csvfile)\n",
    "        self.tensor = pd.read_csv(tensors_csvfile)\n",
    "    \n",
    "    def name(self):\n",
    "        return 'ChampsDataset'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.tensor.iloc[index]\n",
    "        molecule_name = row.molecule_name\n",
    "        atom_index = row.atom_index\n",
    "        \n",
    "        tensor = np.array([row.XX, row.XY, row.XZ, row.YZ, row.YY, row.YZ, row.ZX, row.ZY, row.ZZ])\n",
    "        \n",
    "        if self.opt.atomtypes == 'element':\n",
    "            xyzfile = os.path.join(self.dataroot, 'structures/%s.xyz' % molecule_name)\n",
    "            atomtypes = []\n",
    "            coords = []\n",
    "            for line in open(xyzfile).readlines()[2:]:\n",
    "                entry = line.split()\n",
    "                atomtypes.append(entry[0])\n",
    "                coords.append(list(map(float, entry[1:4])))\n",
    "            atomtypes = np.array(atomtypes)\n",
    "            coords = np.array(coords, dtype=np.float32)\n",
    "            cx, cy, cz = coords[atom_index]\n",
    "        else:\n",
    "            m = self.structures[self.structures.molecule_name == molecule_name]\n",
    "            atom = m[m.atom_index == atom_index]\n",
    "            cx = atom.x.values[0]\n",
    "            cy = atom.y.values[0]\n",
    "            cz = atom.z.values[0]\n",
    "            \n",
    "            atomtypes = m.atom.values\n",
    "            coords = np.vstack([x, y, z]).T\n",
    "            cx, cy, cz = coords[atom_index]\n",
    "        \n",
    "        item = {\n",
    "            'atomtypes': atomtypes,\n",
    "            'atomtype': (atomtypes == row.atom).astype(np.int),\n",
    "            'coords': coords.astype(np.float32),\n",
    "            'tensor': tensor.astype(np.float32),\n",
    "            'center': np.array([cx, cy, cz])\n",
    "        }\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13413825df9a405ebfcbb2f1c67f1c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(Label(value='levels:'), FloatSlider(value=0.4, max=1.0, step=0.00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipyvolume as ipv\n",
    "\n",
    "dataset = ChampsDataset()\n",
    "dataset.initialize(opt)\n",
    "grid = dataset[0]['channels'][2] # C  channel\n",
    "ipv.quickvolshow(grid, level=[0.4, 1],  opacity=0.1, level_width=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChampsDatasetDataLoader\n",
      "dataset [ChampsDataset] was created\n",
      "#training tensors = 153353\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils import data\n",
    "\n",
    "def CreateDataset(opt):\n",
    "    dataset = None\n",
    "    dataset = ChampsDataset()\n",
    "    print(\"dataset [%s] was created\" % (dataset.name()))\n",
    "    dataset.initialize(opt)\n",
    "    return dataset\n",
    "\n",
    "class ChampsDatasetDataLoader:\n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.dataset = CreateDataset(opt)\n",
    "        self.dataloader = data.DataLoader(\n",
    "            self.dataset, batch_size=opt.batch_size,\n",
    "            shuffle=not opt.serial_batches, num_workers=int(opt.nThreads))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i*opt.batch_size >= self.opt.max_dataset_size: break\n",
    "            yield data\n",
    "\n",
    "    def name(self): return 'ChampsDatasetDataLoader'\n",
    "    def load_data(self): return self\n",
    "    def __len__(self): return min(len(self.dataset), self.opt.max_dataset_size)\n",
    "\n",
    "def CreateDataLoader(opt):\n",
    "    data_loader = ChampsDatasetDataLoader()\n",
    "    print(data_loader.name())\n",
    "    data_loader.initialize(opt)\n",
    "    return data_loader\n",
    "\n",
    "data_loader = CreateDataLoader(opt)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "print('#training tensors = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import functools\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_normal(m.weight.data, gain=0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.xavier_normal(m.weight.data, gain=0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def weights_init_orthogonal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.orthogonal(m.weight.data, gain=1)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.orthogonal(m.weight.data, gain=1)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def init_weights(net, init_type='normal'):\n",
    "    print('initialization method [%s]' % init_type)\n",
    "    if init_type == 'normal':\n",
    "        net.apply(weights_init_normal)\n",
    "    elif init_type == 'xavier':\n",
    "        net.apply(weights_init_xavier)\n",
    "    elif init_type == 'kaiming':\n",
    "        net.apply(weights_init_kaiming)\n",
    "    elif init_type == 'orthogonal':\n",
    "        net.apply(weights_init_orthogonal)\n",
    "    else:\n",
    "        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm3d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm3d, affine=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "    if opt.lr_policy == 'lambda':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "def print_network(net, opt):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    if opt.atomtypes == 'element':\n",
    "        print(summary(net, (5, opt.nx, opt.ny, opt.nz)))\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "\n",
    "class BaseModel():\n",
    "    def name(self): return 'BaseModel'\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        self.opt = opt\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.isTrain = opt.isTrain\n",
    "        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n",
    "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
    "\n",
    "    def set_input(self, input): self.input = input\n",
    "    def forward(self): pass\n",
    "    def test(self): pass\n",
    "    def get_image_paths(self): pass\n",
    "    def optimize_parameters(self): pass\n",
    "    def get_current_visuals(self): return self.input\n",
    "    def get_current_errors(self): return {}\n",
    "    def save(self, label): pass\n",
    "\n",
    "    # helper saving function that can be used by subclasses\n",
    "    def save_network(self, network, network_label, epoch_label, gpu_ids):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        torch.save(network.cpu().state_dict(), save_path)\n",
    "        if len(gpu_ids) and torch.cuda.is_available(): network.cuda(gpu_ids[0])\n",
    "\n",
    "    # helper loading function that can be used by subclasses\n",
    "    def load_network(self, network, network_label, epoch_label):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.save_dir, save_filename)\n",
    "        network.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    # update learning rate (called once every epoch)\n",
    "    def update_learning_rate(self):\n",
    "        for scheduler in self.schedulers: scheduler.step()\n",
    "        lr = self.optimizers[0].param_groups[0]['lr']\n",
    "        print('learning rate = %.7f' % lr)\n",
    "        \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple\n",
      "initialization method [normal]\n",
      "---------- Networks initialized -------------\n",
      "SimpleNetworkGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): SimpleBlock(\n",
      "      (conv1): Conv3d(5, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (pool): AdaptiveMaxPool3d(output_size=(24, 24, 24))\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (1): SimpleBlock(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (pool): AdaptiveMaxPool3d(output_size=(12, 12, 12))\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (2): SimpleBlock(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (pool): AdaptiveMaxPool3d(output_size=(6, 6, 6))\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (3): SimpleBlock(\n",
      "      (conv1): Conv3d(256, 512, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n",
      "      (pool): AdaptiveMaxPool3d(output_size=(3, 3, 3))\n",
      "      (activation): ReLU(inplace)\n",
      "    )\n",
      "    (4): Flatten()\n",
      "    (5): Linear(in_features=13824, out_features=4096, bias=True)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Linear(in_features=4096, out_features=512, bias=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): Linear(in_features=512, out_features=9, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 64, 47, 47, 47]          40,064\n",
      "            Conv3d-2       [-1, 64, 47, 47, 47]         512,064\n",
      " AdaptiveMaxPool3d-3       [-1, 64, 24, 24, 24]               0\n",
      "              ReLU-4       [-1, 64, 24, 24, 24]               0\n",
      "       SimpleBlock-5       [-1, 64, 24, 24, 24]               0\n",
      "            Conv3d-6      [-1, 128, 24, 24, 24]       1,024,128\n",
      "            Conv3d-7      [-1, 128, 24, 24, 24]       2,048,128\n",
      " AdaptiveMaxPool3d-8      [-1, 128, 12, 12, 12]               0\n",
      "              ReLU-9      [-1, 128, 12, 12, 12]               0\n",
      "      SimpleBlock-10      [-1, 128, 12, 12, 12]               0\n",
      "           Conv3d-11      [-1, 256, 12, 12, 12]       4,096,256\n",
      "           Conv3d-12      [-1, 256, 12, 12, 12]       8,192,256\n",
      "AdaptiveMaxPool3d-13         [-1, 256, 6, 6, 6]               0\n",
      "             ReLU-14         [-1, 256, 6, 6, 6]               0\n",
      "      SimpleBlock-15         [-1, 256, 6, 6, 6]               0\n",
      "           Conv3d-16         [-1, 512, 6, 6, 6]      16,384,512\n",
      "           Conv3d-17         [-1, 512, 6, 6, 6]      32,768,512\n",
      "AdaptiveMaxPool3d-18         [-1, 512, 3, 3, 3]               0\n",
      "             ReLU-19         [-1, 512, 3, 3, 3]               0\n",
      "      SimpleBlock-20         [-1, 512, 3, 3, 3]               0\n",
      "          Flatten-21                [-1, 13824]               0\n",
      "           Linear-22                 [-1, 4096]      56,627,200\n",
      "             ReLU-23                 [-1, 4096]               0\n",
      "           Linear-24                  [-1, 512]       2,097,664\n",
      "             ReLU-25                  [-1, 512]               0\n",
      "           Linear-26                    [-1, 9]           4,617\n",
      "================================================================\n",
      "Total params: 123,795,401\n",
      "Trainable params: 123,795,401\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.98\n",
      "Forward/backward pass size (MB): 163.90\n",
      "Params size (MB): 472.24\n",
      "Estimated Total Size (MB): 638.12\n",
      "----------------------------------------------------------------\n",
      "None\n",
      "Total number of parameters: 123795401\n",
      "-----------------------------------------------\n",
      "model [SimpleModel] was created\n"
     ]
    }
   ],
   "source": [
    "# network definition\n",
    "class SimpleBlock(nn.Module):\n",
    "    def __init__(self, in_size, in_channel, out_channel):\n",
    "        super(SimpleBlock, self).__init__()\n",
    "        out_size = int(in_size/2)\n",
    "        self.conv1 = nn.Conv3d(in_channel, out_channel, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv3d(out_channel, out_channel, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool3d((out_size, out_size, out_size))\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv2(self.conv1(x)))\n",
    "        return self.activation(x)\n",
    "\n",
    "class SimpleNetworkGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, norm_layer=nn.BatchNorm3d, use_dropout=False, \n",
    "                 n_blocks=6, gpu_ids=[], padding_type='reflect', nx=48):\n",
    "        super(SimpleNetworkGenerator, self).__init__()\n",
    "        self.gpu_ids = gpu_ids\n",
    "        features = [SimpleBlock(48, 5, 64),\n",
    "                    SimpleBlock(24, 64, 128),\n",
    "                    SimpleBlock(12, 128, 256),\n",
    "                    SimpleBlock(6, 256, 512)]\n",
    "        head = [Flatten(),\n",
    "                nn.Linear(13824, 4096),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(4096, 512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(512, 9)]\n",
    "        model = features + head\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "# model definition\n",
    "class SimpleModel(BaseModel):\n",
    "    def name(self):\n",
    "        return 'SimpleModel'\n",
    "    \n",
    "    def define_network(self, input_nc, model='simple', norm='batch', \n",
    "                       use_dropout=False, init_type='normal', gpu_ids=[], nx=48):\n",
    "        net = None\n",
    "        use_gpu = len(gpu_ids) > 0\n",
    "        norm_layer = get_norm_layer(norm_type=norm)\n",
    "        \n",
    "        if use_gpu:\n",
    "            assert(torch.cuda.is_available())\n",
    "\n",
    "        net = SimpleNetworkGenerator(input_nc, norm_layer=norm_layer, \n",
    "                                     use_dropout=use_dropout, gpu_ids=gpu_ids,\n",
    "                                     nx=nx)\n",
    "        if len(gpu_ids) > 0:\n",
    "            net.cuda(gpu_ids[0])\n",
    "        init_weights(net, init_type=init_type)\n",
    "        return net\n",
    "\n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "\n",
    "        self.net = self.define_network(input_nc=opt.input_nc, model=opt.model, \n",
    "                                       gpu_ids=opt.gpu_ids, init_type=opt.init_type,\n",
    "                                       nx=opt.nx)\n",
    "\n",
    "        if not self.isTrain or opt.continue_train:\n",
    "            which_epoch = opt.which_epoch\n",
    "            self.load_network(self.net, 'Simple', which_epoch)\n",
    "\n",
    "        if self.isTrain:\n",
    "            self.criterion1 = torch.nn.MSELoss()\n",
    "            self.criterion2 = torch.nn.L1Loss()\n",
    "            if opt.optimizer == 'sgd':\n",
    "                self.optimizer = torch.optim.SGD(self.net.parameters(), lr=opt.lr, momentum=opt.momentum)\n",
    "            elif opt.optimizer == 'adam':\n",
    "                self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
    "                                                  lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            else:\n",
    "                 raise ValueError\n",
    "            self.optimizers = []\n",
    "            self.schedulers = []\n",
    "            self.optimizers.append(self.optimizer)\n",
    "            for optimizer in self.optimizers:\n",
    "                self.schedulers.append(get_scheduler(optimizer, opt))\n",
    "\n",
    "        print('---------- Networks initialized -------------')\n",
    "        print_network(self.net, opt)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "    def set_input(self, input):\n",
    "        channels = input['channels']\n",
    "        tensor = input['tensor']\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            channels = channels.cuda(self.gpu_ids[0], non_blocking=True)\n",
    "            tensor = tensor.cuda(self.gpu_ids[0], non_blocking=True)\n",
    "        self.input = channels\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    def forward(self):\n",
    "        self.preds = self.net(self.input)\n",
    "    \n",
    "    def backward(self):\n",
    "        #loss = self.criterion(self.preds, self.tensor)\n",
    "        loss = self.criterion1(self.preds, self.tensor) + self.criterion2(self.preds, self.tensor)\n",
    "        loss.backward()\n",
    "        self.loss = loss\n",
    "    \n",
    "    def optimize_parameters(self):\n",
    "        self.forward()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def test(self):\n",
    "        self.forward()\n",
    "\n",
    "    def save(self, label):\n",
    "        self.save_network(self.net, 'SIMPLE', label, self.gpu_ids)\n",
    "\n",
    "    def load(self, label):\n",
    "        self.load_network(self.net, 'SIMPLE', label)\n",
    "        \n",
    "def create_model(opt):\n",
    "    model = None\n",
    "    print(opt.model)\n",
    "    if opt.model == 'simple':\n",
    "        model = SimpleModel()\n",
    "    else:\n",
    "        raise ValueError(\"Model [%s] not recognized.\" % opt.model)\n",
    "    model.initialize(opt)\n",
    "    print(\"model [%s] was created\" % (model.name()))\n",
    "    return model\n",
    "\n",
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 624/19170 [10:38<5:16:23,  1.02s/it, loss=1.83e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the latest model (epoch 1, total_steps 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 1249/19170 [21:18<5:04:41,  1.02s/it, loss=586]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the latest model (epoch 1, total_steps 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 1480/19170 [25:15<5:00:28,  1.02s/it, loss=3.55e+3]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sunhwan/miniconda3/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-df5847acfe3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mepoch_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_latest_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5349132b5645>\u001b[0m in \u001b[0;36moptimize_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5349132b5645>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m#loss = self.criterion(self.preds, self.tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    iter_data_time = time.time()\n",
    "    epoch_iter = 0\n",
    "\n",
    "    with tqdm(total=int(dataset_size/opt.batch_size)+1) as pbar:\n",
    "        for i, data in enumerate(dataset):\n",
    "            iter_start_time = time.time()\n",
    "            if total_steps % opt.print_freq == 0:\n",
    "                t_data = iter_start_time - iter_data_time\n",
    "            total_steps += opt.batch_size\n",
    "            epoch_iter += opt.batch_size\n",
    "            model.set_input(data)\n",
    "            model.optimize_parameters()\n",
    "\n",
    "            if total_steps % opt.save_latest_freq == 0:\n",
    "                print('saving the latest model (epoch %d, total_steps %d)' %\n",
    "                      (epoch, total_steps))\n",
    "                model.save('latest')\n",
    "\n",
    "            iter_data_time = time.time()\n",
    "\n",
    "            pbar.set_postfix(loss=model.loss.item())\n",
    "            pbar.update()\n",
    "            \n",
    "    if epoch % opt.save_epoch_freq == 0:\n",
    "        print('saving the model at the end of epoch %d, iters %d' %\n",
    "              (epoch, total_steps))\n",
    "        model.save('latest')\n",
    "        model.save(epoch)\n",
    "\n",
    "    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "    model.update_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "atomtypes: element\n",
      "batch_size: 1\n",
      "checkpoints_dir: ./checkpoints\n",
      "dataroot: ../../data\n",
      "gpu_ids: [0]\n",
      "grid_func: None\n",
      "grid_size: 9.6\n",
      "grid_spacing: 0.2\n",
      "init_type: normal\n",
      "input_nc: 5\n",
      "isTrain: False\n",
      "max_dataset_size: inf\n",
      "model: simple\n",
      "nThreads: 6\n",
      "name: experiment_name\n",
      "norm: instance\n",
      "nx: 47\n",
      "ny: 47\n",
      "nz: 47\n",
      "phase: test\n",
      "rotate: 0.0\n",
      "serial_batches: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n"
     ]
    }
   ],
   "source": [
    "opt_test = TestOptions().parse('--dataroot ../../data --grid_size 9.6 --grid_spacing 0.2 \\\n",
    "                                --batch_size 1 --rotate 0'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChampsDatasetDataLoader\n",
      "dataset [ChampsDataset] was created\n",
      "#training tensors = 1533537\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "data_loader = CreateDataLoader(opt_test)\n",
    "dataset = data_loader.load_data()\n",
    "dataset_size = len(data_loader)\n",
    "print('#training tensors = %d' % dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(dataset))\n",
    "model.set_input(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17.5235,  0.2903,  1.4183,  0.0371, 16.3074, -0.1859,  2.2562,  0.3406,\n",
       "         10.8884]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28.6384, -2.3927,  2.9203,  1.8576, 23.1132,  1.8576,  5.4734, -2.5099,\n",
       "         31.2959]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
