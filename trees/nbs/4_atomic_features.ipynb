{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../data/*.csv': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path('../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH/'train.csv')\n",
    "test = pd.read_csv(PATH/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_atomtype(df, s):\n",
    "    # https://www.kaggle.com/jazivxt/all-this-over-a-dog\n",
    "    df['atom1'] = df['type'].map(lambda x: str(x)[2])\n",
    "    df['atom2'] = df['type'].map(lambda x: str(x)[3])\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    for i in range(4):\n",
    "        df['type'+str(i)] = lbl.fit_transform(df['type'].map(lambda x: str(x)[i]))\n",
    "\n",
    "    df = pd.merge(df, s.rename(columns={'atom_index':'atom_index_0', 'x':'x0', 'y':'y0', 'z':'z0', 'atom':'atom1'}), how='left', on=['molecule_name', 'atom_index_0', 'atom1'])\n",
    "    df = pd.merge(df, s.rename(columns={'atom_index':'atom_index_1', 'x':'x1', 'y':'y1', 'z':'z1', 'atom':'atom2'}), how='left', on=['molecule_name', 'atom_index_1', 'atom2'])\n",
    "    return df\n",
    "\n",
    "def feature_pair_geometry(df):\n",
    "    p0 = df[['x0', 'y0', 'z0']].values\n",
    "    p1 = df[['x1', 'y1', 'z1']].values\n",
    "    r = np.linalg.norm(p0 - p1, axis=1)\n",
    "    df['dist'] = r\n",
    "\n",
    "    for agg in ['min', 'max', 'mean']:\n",
    "        tmp = eval('df.groupby([\"type\"], as_index=False).dist.' + agg + '()')\n",
    "        tmp.rename(columns={\"dist\":agg + \"_dist\"}, inplace=True)\n",
    "        df = pd.merge(df, tmp, how='left', on=['type'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    # somewhere from kaggle kernel\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem)\n",
    " / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_basic(df):\n",
    "    structures = pd.read_csv(PATH/'structures.csv')\n",
    "    df = feature_atomtype(df, structures)\n",
    "    df = feature_pair_geometry(df)\n",
    "    df = reduce_mem_usage(df)\n",
    "    del structures\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 417.58 Mb (48.9% reduction)\n",
      "Mem. usage decreased to 215.05 Mb (48.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = feature_basic(train)\n",
    "test = feature_basic(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>atom1</th>\n",
       "      <th>atom2</th>\n",
       "      <th>type0</th>\n",
       "      <th>type1</th>\n",
       "      <th>...</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>z0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>dist</th>\n",
       "      <th>min_dist</th>\n",
       "      <th>max_dist</th>\n",
       "      <th>mean_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807602</td>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.060901</td>\n",
       "      <td>1.247942</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>1.513358</td>\n",
       "      <td>1.969340</td>\n",
       "      <td>1.774895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>1.513358</td>\n",
       "      <td>1.969340</td>\n",
       "      <td>1.774895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "      <td>H</td>\n",
       "      <td>H</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>1.513358</td>\n",
       "      <td>1.969340</td>\n",
       "      <td>1.774895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807404</td>\n",
       "      <td>H</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>1.060901</td>\n",
       "      <td>1.247942</td>\n",
       "      <td>1.092900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "0   0  dsgdb9nsd_000001             1             0  1JHC   \n",
       "1   1  dsgdb9nsd_000001             1             2  2JHH   \n",
       "2   2  dsgdb9nsd_000001             1             3  2JHH   \n",
       "3   3  dsgdb9nsd_000001             1             4  2JHH   \n",
       "4   4  dsgdb9nsd_000001             2             0  1JHC   \n",
       "\n",
       "   scalar_coupling_constant atom1 atom2  type0  type1    ...            x0  \\\n",
       "0                 84.807602     H     C      0      0    ...      0.002150   \n",
       "1                -11.257000     H     H      1      0    ...      0.002150   \n",
       "2                -11.254800     H     H      1      0    ...      0.002150   \n",
       "3                -11.254300     H     H      1      0    ...      0.002150   \n",
       "4                 84.807404     H     C      0      0    ...      1.011731   \n",
       "\n",
       "         y0        z0        x1        y1        z1      dist  min_dist  \\\n",
       "0 -0.006031  0.001976 -0.012698  1.085804  0.008001  1.091953  1.060901   \n",
       "1 -0.006031  0.001976  1.011731  1.463751  0.000277  1.783120  1.513358   \n",
       "2 -0.006031  0.001976 -0.540815  1.447527 -0.876644  1.783147  1.513358   \n",
       "3 -0.006031  0.001976 -0.523814  1.437933  0.906397  1.783157  1.513358   \n",
       "4  1.463751  0.000277 -0.012698  1.085804  0.008001  1.091952  1.060901   \n",
       "\n",
       "   max_dist  mean_dist  \n",
       "0  1.247942   1.092900  \n",
       "1  1.969340   1.774895  \n",
       "2  1.969340   1.774895  \n",
       "3  1.969340   1.774895  \n",
       "4  1.247942   1.092900  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/adrianoavelar/bond-calculaltion-lb-0-82\n",
    "def feature_neighbors(s):\n",
    "    i_atom = s['atom_index'].values\n",
    "    i_atom_type = s['atom'].values\n",
    "    p = s[['x', 'y', 'z']].values\n",
    "    m = s['molecule_name'].values\n",
    "    t = np.empty((len(s)+1), dtype=np.object)\n",
    "    t[:len(s)] = s['atom'].values\n",
    "    p_compare = p\n",
    "    m_compare = m\n",
    "    t_compare = t\n",
    "    source_row = np.arange(len(s))\n",
    "    max_atoms = max(s.groupby('molecule_name').atom_index.max().values)\n",
    "    bonds = np.zeros((len(s)+1, max_atoms+1), dtype=np.int8)\n",
    "    bond_dists = np.zeros((len(s)+1, max_atoms+1), dtype=np.float32)\n",
    "    bond_atoms = np.empty((len(s)+1, max_atoms+1), dtype=np.object)\n",
    "    for i in tqdm(range(max_atoms-1)):\n",
    "        p_compare = np.roll(p_compare, -1, axis=0)\n",
    "        m_compare = np.roll(m_compare, -1, axis=0)\n",
    "        t_compare = np.roll(t_compare, -1, axis=0)\n",
    "\n",
    "        mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n",
    "        dists = np.linalg.norm(p - p_compare, axis=1) * mask\n",
    "        r_bond = 3.0\n",
    "\n",
    "        bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n",
    "\n",
    "        source_row = source_row\n",
    "        target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "        target_row = np.where(np.logical_or(target_row > len(s), mask==0), len(s), target_row) #If invalid target, write to dummy row\n",
    "\n",
    "        source_atom = i_atom\n",
    "        target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "        target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n",
    "\n",
    "        target_atom_type = np.where(np.logical_or(target_row > len(s), mask==0), '', t[target_row]) #If invalid target, write to dummy row    \n",
    "        source_atom_type = i_atom_type\n",
    "\n",
    "        bonds[(source_row, target_atom)] = bond\n",
    "        bonds[(target_row, source_atom)] = bond\n",
    "        bond_dists[(source_row, target_atom)] = dists\n",
    "        bond_dists[(target_row, source_atom)] = dists\n",
    "        bond_atoms[(source_row, target_atom)] = target_atom_type\n",
    "        bond_atoms[(target_row, source_atom)] = source_atom_type\n",
    "\n",
    "    bonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\n",
    "    bonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\n",
    "    bond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\n",
    "    bond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n",
    "    bond_atoms = np.delete(bond_atoms, axis=0, obj=-1) #Delete dummy row\n",
    "    bond_atoms = np.delete(bond_atoms, axis=1, obj=-1) #Delete dummy col\n",
    "    \n",
    "    mask = bonds == 1\n",
    "    bond_lengths_mean = [np.mean(row[mask[j]]) for j,row in enumerate(tqdm(bond_dists))]\n",
    "    n_bonds = np.sum(bonds, axis=1)\n",
    "    bond_data = {'n_bonds':n_bonds, 'bond_lengths_mean': bond_lengths_mean }\n",
    "    bond_df = pd.DataFrame(bond_data)\n",
    "    s = s.join(bond_df)\n",
    "    \n",
    "    atom_types = ['C', 'N', 'H', 'O', 'F']\n",
    "    bond_data = {}\n",
    "    for at in atom_types:\n",
    "        bonds_at = np.empty((len(s), max_atoms), dtype=np.int8) \n",
    "        bonds_at[:] = -1\n",
    "        for i in tqdm(range(len(bond_atoms))):\n",
    "            mask = bond_atoms[i,:] == at\n",
    "            atom_j_indices = np.argwhere(mask)\n",
    "            dists = bond_dists[i, mask]\n",
    "            atom_j_sorted = np.argsort(dists)\n",
    "            bonds_at[i, :len(atom_j_sorted)] = atom_j_sorted\n",
    "            \n",
    "        maxatom = np.max(np.sum(bonds_at >= 0, axis=1))\n",
    "        maxatom = 6\n",
    "        for i in range(maxatom):\n",
    "            bond_data['bond_%s_%d' % (at, i)] = bonds_at[:, i]\n",
    "    bond_df = pd.DataFrame(bond_data)\n",
    "    s = s.join(bond_df)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dist(df, s):\n",
    "    atom0_xyz = df[['x0', 'y0', 'z0']].values\n",
    "    atom1_xyz = df[['x1', 'y1', 'z1']].values\n",
    "    for col in tqdm(df.columns):\n",
    "        if col.startswith('bond') and not col.startswith('bond_length'):\n",
    "            xyz = pd.merge(df, s.drop('atom', axis=1).rename(columns={'atom_index':col, 'x':col+'_x', 'y':col+'_y', 'z':col+'_z'}), how='left', on=['molecule_name', col])[[col+'_x', col+'_y', col+'_z']].values\n",
    "            r0 = np.linalg.norm(atom0_xyz - xyz, axis=1)\n",
    "            r1 = np.linalg.norm(atom1_xyz - xyz, axis=1)\n",
    "            df[col+'_r0'] = r0\n",
    "            df[col+'_r1'] = r1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bonds(df1, df2):\n",
    "    structures = pd.read_csv(PATH/'structures.csv')\n",
    "    structures = feature_neighbors(structures)\n",
    "    df1 = pd.merge(df1, structures.drop(['x', 'y', 'z'], axis=1).rename(columns={'atom_index': 'atom_index_0', 'atom':'atom1'}), how='left', on=['molecule_name', 'atom_index_0', 'atom1'])\n",
    "    df1 = pd.merge(df1, structures.drop(['x', 'y', 'z'], axis=1).rename(columns={'atom_index': 'atom_index_0', 'atom':'atom1'}), how='left', on=['molecule_name', 'atom_index_0', 'atom1'])\n",
    "    df2 = pd.merge(df2, structures.drop(['x', 'y', 'z'], axis=1).rename(columns={'atom_index': 'atom_index_0', 'atom':'atom1'}), how='left', on=['molecule_name', 'atom_index_0', 'atom1'])\n",
    "    df2 = pd.merge(df2, structures.drop(['x', 'y', 'z'], axis=1).rename(columns={'atom_index': 'atom_index_0', 'atom':'atom1'}), how='left', on=['molecule_name', 'atom_index_0', 'atom1'])\n",
    "    \n",
    "    df1 = feature_dist(df1, structures)\n",
    "    df2 = feature_dist(df2, structures)\n",
    "\n",
    "    df1 = reduce_mem_usage(df1)\n",
    "    df2 = reduce_mem_usage(df2)\n",
    "    del structures\n",
    "    gc.collect()\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:07<00:00,  3.74it/s]\n",
      "100%|██████████| 2358657/2358657 [00:18<00:00, 126208.41it/s]\n",
      "100%|██████████| 2358657/2358657 [00:28<00:00, 83572.00it/s]\n",
      "100%|██████████| 2358657/2358657 [00:27<00:00, 85844.88it/s]\n",
      "100%|██████████| 2358657/2358657 [00:28<00:00, 82852.90it/s]\n",
      "100%|██████████| 2358657/2358657 [00:26<00:00, 89302.88it/s]\n",
      "100%|██████████| 2358657/2358657 [00:32<00:00, 72001.20it/s]\n",
      "100%|██████████| 86/86 [06:42<00:00, 10.09s/it]\n",
      "100%|██████████| 85/85 [03:29<00:00,  5.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2931.95 Mb (42.4% reduction)\n",
      "Mem. usage decreased to 1682.19 Mb (38.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "train, test = feature_bonds(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmols = {}\\nn_cpu = 4\\nwith Pool(n_cpu) as p:\\n    molecule_names = np.concatenate([train.molecule_name.unique(), test.molecule_name.unique()])\\n    xyzfiles = [Path(PATH/f'structures/{f}.xyz') for f in molecule_names]\\n    n = len(xyzfiles)\\n    with tqdm(total=n) as pbar:\\n        for res in p.imap_unordered(MolFromXYZ_, xyzfiles):\\n            mols[res[0]] = res[1]\\n            pbar.update()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdkit & xyz2mol\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Draw import IPythonConsole #Needed to show molecules\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions #Only needed if modifying defaults\n",
    "DrawingOptions.bondLineWidth=1.8\n",
    "from rdkit.Chem.rdmolops import SanitizeFlags\n",
    "\n",
    "# https://github.com/jensengroup/xyz2mol\n",
    "from xyz2mol import xyz2mol, xyz2AC, AC2mol, read_xyz_file\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "def chiral_stereo_check(mol):\n",
    "    # avoid sanitization error e.g., dsgdb9nsd_037900.xyz\n",
    "    Chem.SanitizeMol(mol, SanitizeFlags.SANITIZE_ALL - SanitizeFlags.SANITIZE_PROPERTIES)\n",
    "    Chem.DetectBondStereochemistry(mol,-1)\n",
    "    # ignore stereochemistry for now\n",
    "    #Chem.AssignStereochemistry(mol, flagPossibleStereoCenters=True, force=True)\n",
    "    #Chem.AssignAtomChiralTagsFromStructure(mol,-1)\n",
    "    return mol\n",
    "\n",
    "def xyz2mol(atomicNumList,charge,xyz_coordinates,charged_fragments,quick):\n",
    "    AC,mol = xyz2AC(atomicNumList,xyz_coordinates)\n",
    "    new_mol = AC2mol(mol,AC,atomicNumList,charge,charged_fragments,quick)\n",
    "    new_mol = chiral_stereo_check(new_mol)\n",
    "    return new_mol\n",
    "\n",
    "def MolFromXYZ(filename):\n",
    "    charged_fragments = True\n",
    "    quick = True\n",
    "    cache_filename = filename.parent/f'{filename.stem}.pkl'\n",
    "    if cache_filename.exists():\n",
    "        return pickle.load(open(cache_filename, 'rb'))\n",
    "    else:\n",
    "        try:\n",
    "            atomicNumList, charge, xyz_coordinates = read_xyz_file(filename)\n",
    "            mol = xyz2mol(atomicNumList, charge, xyz_coordinates, charged_fragments, quick)\n",
    "            pickle.dump(mol, open(cache_filename, 'wb'))\n",
    "        except:\n",
    "            print(filename)\n",
    "    return mol\n",
    "\n",
    "#mol = MolFromXYZ(xyzfiles[1])\n",
    "#m = Chem.MolFromSmiles(Chem.MolToSmiles(mol, allHsExplicit=True)); m\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from tqdm import *\n",
    "from glob import glob\n",
    "\n",
    "def MolFromXYZ_(filename):\n",
    "    return filename.stem, MolFromXYZ(filename)\n",
    "\n",
    "# skipping below for saving memory\n",
    "\"\"\"\n",
    "mols = {}\n",
    "n_cpu = 4\n",
    "with Pool(n_cpu) as p:\n",
    "    molecule_names = np.concatenate([train.molecule_name.unique(), test.molecule_name.unique()])\n",
    "    xyzfiles = [Path(PATH/f'structures/{f}.xyz') for f in molecule_names]\n",
    "    n = len(xyzfiles)\n",
    "    with tqdm(total=n) as pbar:\n",
    "        for res in p.imap_unordered(MolFromXYZ_, xyzfiles):\n",
    "            mols[res[0]] = res[1]\n",
    "            pbar.update()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "def feature_atom(row):\n",
    "    m = MolFromXYZ(PATH/f'structures/{row.molecule_name}.xyz')\n",
    "    #m = mols[row.molecule_name]\n",
    "    atom = m.GetAtomWithIdx(row.atom_index)\n",
    "    prop = {}\n",
    "    nb = [a.GetSymbol() for a in atom.GetNeighbors()] # neighbor atom type symbols\n",
    "    nb_h = sum([_ == 'H' for _ in nb])\n",
    "    nb_o = sum([_ == 'O' for _ in nb])\n",
    "    nb_c = sum([_ == 'C' for _ in nb])\n",
    "    nb_n = sum([_ == 'N' for _ in nb])\n",
    "    nb_f = sum([_ == 'F' for _ in nb])\n",
    "    row['degree'] = atom.GetDegree()\n",
    "    row['hybridization'] = int(atom.GetHybridization())\n",
    "    row['inring'] = int(atom.IsInRing())\n",
    "    row['inring4'] = int(atom.IsInRingSize(4))\n",
    "    row['inring5'] = int(atom.IsInRingSize(5))\n",
    "    row['inring6'] = int(atom.IsInRingSize(6))\n",
    "    row['nb_h'] = nb_h\n",
    "    row['nb_o'] = nb_o\n",
    "    row['nb_c'] = nb_c\n",
    "    row['nb_n'] = nb_n\n",
    "    row['nb_f'] = nb_f\n",
    "    return row\n",
    "\n",
    "def feature_structure_atoms(s):\n",
    "    names = s.molecule_name.unique()\n",
    "    meta = s.iloc[:1].apply(feature_atom, axis=1)\n",
    "    t0 = time.time()\n",
    "    structures = dd.from_pandas(s, npartitions=4*multiprocessing.cpu_count()) \\\n",
    "                    .map_partitions(lambda df: df.apply(feature_atom, axis=1), meta=meta) \\\n",
    "                    .compute(scheduler='processes')\n",
    "    print(f'took {time.time() - t0} sec')\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [08:36<00:00, 13.37s/it]\n",
      "100%|██████████| 60/60 [04:40<00:00,  7.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 5863.91 Mb (33.3% reduction)\n",
      "Mem. usage decreased to 3259.24 Mb (32.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "def feature_bonded_atoms(df, structures):\n",
    "    bc = [col for col in df.columns if col.startswith('bond') and not col.startswith('bond_length') and not col.endswith('r0') and not col.endswith('r1')]\n",
    "    for col in tqdm(bc):\n",
    "        s = structures.drop(['x', 'y', 'z', 'atom'], axis=1).rename(columns={'atom_index': col})\n",
    "        sc = [_ for _ in s.columns if _ != col and _ != 'molecule_name']\n",
    "        s = s.rename(columns={k: col+'_'+k for k in sc})\n",
    "        df = pd.merge(df, s, how='left', on=['molecule_name', col])\n",
    "        for c in sc:\n",
    "            df[col+'_'+c] = df[col+'_'+c].fillna(-1).astype(np.int16)\n",
    "    return df\n",
    "\n",
    "def feature_atoms(df1, df2):\n",
    "    if (PATH/'structures_atoms.csv').exists():\n",
    "        structures = pd.read_csv(PATH/'structures_atoms.csv')\n",
    "        if 'Unnamed: 0' in structures:\n",
    "            structures = structures.drop('Unnamed: 0', axis=1)\n",
    "    else:\n",
    "        structures = pd.read_csv(PATH/'structures.csv')\n",
    "        structures = feature_atoms(structures)\n",
    "        structures.to_csv(PATH/'structures_atoms.csv', index=False)\n",
    "    \n",
    "    df1 = feature_bonded_atoms(df1, structures)\n",
    "    df2 = feature_bonded_atoms(df2, structures)\n",
    "    df1 = reduce_mem_usage(df1)\n",
    "    df2 = reduce_mem_usage(df2)\n",
    "    del structures\n",
    "    gc.collect()\n",
    "    return df1, df2\n",
    "\n",
    "train, test = feature_atoms(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(PATH/'train_tmp.pkl')\n",
    "test.to_pickle(PATH/'test_tmp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import gc\n",
    "from pathlib import Path\n",
    "PATH = Path('../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(PATH/'train_tmp.pkl')\n",
    "test = pd.read_pickle(PATH/'test_tmp.pkl')\n",
    "# n = 4658147\n",
    "# train_index = np.array([i for i in range(n) if i % 10 != 0])\n",
    "# test_index = np.array([i for i in range(n) if i % 10 == 0])\n",
    "# X_train = pd.read_pickle(PATH/'train_tmp.pkl').iloc[train_index].fillna(-1)\n",
    "# X_test = pd.read_pickle(PATH/'train_tmp.pkl').iloc[test_index].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['id', 'molecule_name', 'atom_index_0', 'atom_index_1', 'atom1', 'atom2'] + ['x0', 'y0', 'z0', 'x1', 'y1', 'z1']\n",
    "for i in range(6):\n",
    "    excluded += ['bond_%s_%d_x' % (t, i) for t in ['C', 'H', 'O', 'N', 'F']]\n",
    "    excluded += ['bond_%s_%d_y' % (t, i) for t in ['C', 'H', 'O', 'N', 'F']]\n",
    "col = [c for c in train.columns if c not in ['scalar_coupling_constant'] + excluded]\n",
    "#reg = ensemble.ExtraTreesRegressor(n_jobs=-1, n_estimators=5, random_state=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(train[col], train['scalar_coupling_constant'], test_size=0.2)\n",
    "# y_train = X_train['scalar_coupling_constant']\n",
    "# y_test = X_test['scalar_coupling_constant']\n",
    "# X_train = X_train[col]\n",
    "# X_test = X_test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 105.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,\n",
       "          oob_score=False, random_state=4, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = ensemble.ExtraTreesRegressor(n_jobs=-1, n_estimators=5, random_state=4, verbose=1)\n",
    "reg.fit(X_train.drop(['type'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 out of  20 | elapsed:    5.1s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test.drop('type', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/uberkinder/efficient-metric\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.49973494851025835"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_mean_log_mae(y_test, y_pred, X_test.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1JHC 1.5427883354914589\n",
      "2JHH 0.34006164474682826\n",
      "1JHN 1.104168635526701\n",
      "2JHN 0.5175124004509403\n",
      "2JHC 0.7007759737563171\n",
      "3JHH 0.4122990452001901\n",
      "3JHC 0.6355013892338905\n",
      "3JHN 0.33343774004197446\n"
     ]
    }
   ],
   "source": [
    "for t in train.type.unique():\n",
    "    idx = X_test.type == t\n",
    "    print(t, (y_test[idx] - y_pred[idx]).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunhwan/miniconda3/lib/python3.7/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=-1,\n",
       "          oob_score=False, random_state=4, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train with all data\n",
    "reg.fit(train[col].drop('type', axis=1), train['scalar_coupling_constant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "test['scalar_coupling_constant']  = reg.predict(test[col].drop('type', axis=1))\n",
    "test[['id', 'scalar_coupling_constant']].to_csv('submission.csv', index=False) #float_format='%.9f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission.csv' target='_blank'>submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/sunhwan/work/kaggle/champs-scalar-coupling/nbs/submission.csv"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score: ~0.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
